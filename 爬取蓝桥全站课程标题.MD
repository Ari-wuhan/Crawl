实现一个爬虫程序的完整闭环：
* 获取数据
* 解析数据
* 存储数据
# requests 库安装

* 第一步：安装 requests
我们首先打开桌面上的 Xfce 终端，在我们安装 requests 库之前，先更新一下 pip 。在终端命令行执行如下命令：

sudo pip install -U pip 
copy
然后通过下面的命令对 requests 进行安装，注意实验楼提供的 Python 环境中 requests 已经预安装，所以使用线上环境的同学可以跳过这一步：

sudo pip install requests
copy
* 第二步：在 Python 源码文件中导入该扩展
首先创建一个 Python 文件，除使用下述命令创建外，也可以使用鼠标进行操作：

mkdir demo1
cd demo1
touch crawl.py
copy
创建完毕，目录结果如下图所示：

运行结果

在 crawl.py 中输入如下代码，测试 requests 扩展是否安装成功，编写代码之后，注意保存。

> import 表示导入模块，后面跟模块名。
import requests
copy
在终端运行 crawl.py 文件，如果未出现任何报错信息，表示 requests 扩展已经安装成功：

python3 crawl.py
copy
>注意：上面的命令可能会出现 CryptographyDeprecationWarning 警告信息，这不是报错，可以忽略它。*
>如果要消除警告信息，可以执行如下命令：
>cd /usr/local/lib/python3.5/dist-packages/cryptography     
copy
然后使用编辑器打开 __init__.py 文件，删除 42 行及其后面的代码，也就是最后一个 if 判断语句块。


# 爬虫初体验

首先将以下代码写入到 crawl.py 文件中，注意看注释哦：

* import requests

 声明一个变量url，并将其赋值为 https://xxxx （图片地址）
url = "https://dn-simplecloud.shiyanlou.com/courses/uid214893-20200325-1585103411810?imageView2/2/h/150/q/100"
#请求网络地址
res = requests.get(url)
#获取请求到的二进制数据流
* img = res.content

 打开并写入文件
* with open("./image.png", "wb") as f:
    f.write(img)
copy
下面来详细看一下上述代码涉及的知识点：

> requests.get 是 requests 内置的一个方法，get 方法会发起一个 GET 请求，去请求网络上的一个地址，即我们要获取的图片地址。
res.content 中，res 是一个新变量，用于保存 requests 请求到的数据，你可以通过 print(res) 对其进行直接打印， res.content 表示获取二进制文件流，主要用于图片，视频，音频等富文本载体的文件。
with... as ... with 上下文代码段，后续会有详细说明，该关键字可以保证文件一定会被关闭。
然后在终端运行 crawl.py 文件。

python3 crawl.py

代码运行完毕，注意文件目录结构中出现一张名为 image.png 的图片，该图片即为下载之后的图片：

图片描述

* 简化代码
上面的代码太长，太繁琐怎么办，那就让我们一起来简化吧：

import requests

res = requests.get(
    "https://dn-simplecloud.shiyanlou.com/courses/uid214893-20200325-1585103411810?imageView2/2/h/150/q/100")

with open("./image.png", "wb") as f:
    f.write(res.content)


# re 模块与正则表达式
## re 模块与正则表达式初次相遇
学习 re 模块无法与正则表达式分开进行，所以本小节将对这两部分进行一个整体的串讲。首先我们学习 re.match 方法。

这里补充一下，re 模块核心就是解析字符串，获取字符串里面的局部内容。re.match 方法就是从字符串的起始位置开始匹配，匹配成功返回匹配对象，失败返回 None。re.match 方法的参数说明请注意下述代码注释位置。

将如下代码覆盖写入 /home/project/demo1/crawl.py 文件：
>
import re  # 导入re模块

re.match("要匹配的正则表达式","待匹配的字符串")

result1 = re.match("实", "实验楼Python爬虫实战课程-梦想橡皮擦")

result2 = re.match("验", "实验楼Python爬虫实战课程-梦想橡皮擦")

print(result1)

print(result2)


代码输出如下图所示，第一个 result1 由于从起始位置匹配到数据，故返回一个对象，第二个由于“验”字并不在第一个位置，所以返回 None ：

图片描述

看到这里，你肯定有疑问，看上去 re 模块与正则表达式的使用特别像是字符串比对查找，并没有看出特别的地方，接下来请看下述代码：

import re

result1 = re.match("实\w", "实验楼Python爬虫实战课程")

result2 = re.match("\w验", "实验楼Python爬虫实战课程")

print(result1)

print(result2)




我只做了非常小的一点改变，在上述代码中增加了一个特别的字符 \w 就发现两次都匹配成功了。这表示从字符串起始的位置都可以匹配到数据，这里的 \w 就是正则表达式中的元字符。

对于元字符有非常多的内容需要补充，建议大家可以通过搜索引擎扩展知识，例如在 这个网站 进行学习。

在我们后续课程中使用比较多的是这三个符号 . * ? ，如果还有需要特别注意的元字符，那就是 () 与 + ，请重点学习这部分内容。

## re 模块与正则表达式匹配网页标题
接下来我将提供一段 HTML 代码，通过这段代码的解析，你将初步了解如何使用 re 模块配合正则表达式去解析一般的网页结构。

### HTML 参考代码如下：

<p>
<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>实验楼Python爬虫实战课程</title>
  </head>
  <body>
    <h2>梦想橡皮擦老师的课程</h2>
  </body>
</html>
</p>

我们要完成的效果是匹配出网页的标题，匹配出网页中 h2 标签的文字。将以下代码覆盖写入 crawl.py 文件中：
'''
import re
# 声明待爬取的字符串'

html =''' 
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>实验楼Python爬虫实战课程</title>
</head>
<body>
    <h2>梦想橡皮擦老师的课程</h2>
</body>
</html>
"""
'''

/# 通过 search 匹配数据
result = re.search("<title>(.*)</title>", html)
print(result)
print(result.group(1))
copy
上述代码的一些重要知识点如下：

re.search 方法第一个参数也是正则表达式，第二个参数是待匹配的字符串，在本代码中是模拟的一段 HTML 代码。
正则表达式中出现了 .， *， ()，注意都是英文输入法状态输入的， . 表示匹配任意字符，* 表示匹配前面的字符零次或者多次，括号的用途是为了后面的提取。
最后一行代码打出了网页的标题，使用的代码是 result.group(1) 表示获取正则表达式中第一个括号的内容，你可以尝试将数字换成 2，看一下报错内容。
通过命令行运行之后，效果如下图所示：

图片描述

接下来我们在获取一下 h2 标签内的文字，代码如下：

/# 只展示不同部分
result = re.search("<h2>(.*)</h2>", html)
print(result)
print(result.group(1))
copy
我只将正则表达式部分的 title 切换成 h2 即可匹配成功。

下面我们在提高一下难度，通过 HTML 标签我们将 h2 的颜色设置为红色，代码修改如下（只展示差异部分）：

<h2 style="color:red;">梦想橡皮擦老师的课程</h2>
copy
这时，我们使用刚才的正则表达式就无法获取到文字内容了，因为 <h2>(.*)</h2> 无法正确匹配到上述 HTML 代码，下面做出相应的调整，注意这里依旧采用 . 与 * 的配合即可。

/# 注意在h2的后面存在一个空格，之后是.*表示匹配任意字符，在之后是一个>，用于和后面的内容做一下区分。
result = re.search("<h2 .*>(.*)</h2>", html)
print(result)
print(result.group(1))
copy
上述代码大家需要加深理解，由于篇幅的关系，没有办法为大家详细的把正则表达式全部内容都展开，所以在有限的案例中一定要找到正则表达式与 re 模块配合使用的感觉。

上文还有一个细节需要注意 . 无法匹配空格。

# re 模块解析实验楼课程页
通过 request 获取待解析网页
对于任何一个爬虫程序来说，第一步都是获取待解析的内容，本实验要获取的网页地址为[实验楼课程页](https://lanqiao-horuikidi.vercel.app/)。

我们在前文中已经把要使用的技术做好相应的铺垫，使用如下代码获取即可。
'''
import requests

headers = {
    "user-agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36"
}

url = "https://lanqiao-horuikidi.vercel.app/"

res = requests.get(url=url, headers=headers)
print(res.text)
'''
代码说明与结果展示：

注意最后一行代码打印的内容为 res.text 它表示打印网页返回的文档信息。
代码运行之后，你会看到如下一堆内容的输出，不用担心，稍后就会解析清楚。



这里打印出的内容，就是访问网页返回的网页源码，而通过 re 模块配合正则表达，就可以实现对它的解析，例如，把网页的标题匹配出来。



#导入模块
import requests
import re

headers = {
    "user-agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36"
}

url = "https://lanqiao-horuikidi.vercel.app/"

#发送数据请求

res = requests.get(url=url, headers=headers)

html_data = res.text

#通过正则匹配数据

result = re.search("<title>(.*)</title>", html_data)

print(result.group(1))


代码运行结果：



## 这里就是上文的一个整体结合了，它的逻辑如下：

 * 1.通过 requests 库获取网页源码。

* 2.通过 re 模块使用正则表达式解析网页。

* 3.输出我们得到的数据。

# 解析所有的课程标题
网页标题已经解析出来，继续增加难度，将网页中所有的课程标题解析出来，这里需要在学习 re 模块下的一个方法。

re.findall 该方法看到名字就能猜出它的含义，匹配所有的数据。它的标准格式如下：

* a = re.findall("匹配规则", "待匹配字符串")

方法先放在这个位置等待使用，下面你要用人眼看的方式去找到待匹配的数据所在位置。你可以使用谷歌或者火狐浏览器的开发者工具进行分析，如下图所示：

图片描述

也可以在浏览器右键查看源码，通过搜索的方式找到数据所在区域，如下图所示：

图片描述

不管通过何种方式，都要了解到数据所在区域的 HTML 代码格式，只有看到，才能爬取到。（注意这句话的潜在价值，爬虫的真谛。）

接下来就可以进行实际的编码了。

HTML 代码结构：

<h6 title="新手入门指南之玩转实验楼" class="course-name" data-v-585c8a00>
  ..........
</h6>
copy
正则表达式（可以进行比对着些，也可以通过搜索引擎搜索【在线正则表达式匹配】使用工具完成）。

<h6 title="(.*)" class="course-name"

Python 爬虫代码如下：

import requests
import re

headers = {
    "user-agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36"
}
url = "https://lanqiao-horuikidi.vercel.app/"
res = requests.get(url=url, headers=headers)
html_data = res.text
results = re.findall('<h6 title="(.*)" class="course-name"', html_data)
print(results)

代码说明：

findall 方法部分，注意匹配规则中如果出现了双引号，为了方式代码报错，需要在最外层使用单引号进行区分。
运行结果：

图片描述

代码运行完毕，成功得到本页所有课程标题。

## 通过 for 循环遍历实验楼全部课程列表
截止到上述位置，你已经实现了单页课程标题的爬取，在这里留个小的扩展实验，你可以通过扩展将实验楼所有课程页面的标题获取到。提示：需要找到分页的规律。

测试网址的 URL 规律如下（真实环境需要自行检测）：

https://lanqiao-horuikidi.vercel.app/index.html
https://lanqiao-horuikidi.vercel.app/index_2.html
https://lanqiao-horuikidi.vercel.app/index_3.html
https://lanqiao-horuikidi.vercel.app/index_4.html
... ...


通过循环 for 循环，依次拼接 URL 地址即可实现。

# 爬取数据存储
下面就是本实验的最后一个步骤了，将爬取到的数据存储到文件中，在本实验中你可以将获取到的数据进行一些简单的格式化操作，再存入文件中，具体代码如下：

import requests
import re

headers = {
    "user-agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36"
}
url = "https://lanqiao-horuikidi.vercel.app/"
res = requests.get(url=url, headers=headers)
html_data = res.text
# 匹配数据，获取所有的课程标题
results = re.findall('<h6 title="(.*)" class="course-name"', html_data)

for item in results:
    # 格式化字符串
    new_str = "课程名：{item}\n".format(item=item)
    # 将格式化之后的数据写入文件
    with open("./data.txt", "a", encoding="utf-8") as f:
        f.write(new_str)
copy
代码运行完毕，会在当前文件目录生成一个 data.txt 的文件，文件内容如图：

图片描述

上述代码中，with 上一实验已经介绍过，是 Python 中一种语法结构，with 后面的表达式 open('./data.txt') 返回是一个 file 类型的变量，后面用 as 可以理解成给起了一个别名叫做 f，其它名称都可以。在 with 语句块中就可以使用这个变量操作文件，执行 with 这个结构之后， f 会自动关闭。

with 代码如果使用一般写法，类似下述代码：

file = open("data.txt")

try:
    data = file.write("abc")
finally:
    file.close()

示例中出现代码段 open('./data.txt','a',encoding="utf-8")，里面的各参数含义如下：

'./data.txt' 存储的文件名，

其中 ./ 表示当前目录。
a 打开文件的模式：只读 r，写入 w，追加 a 等，
具体可以参考：[学习地址](https://www.runoob.com/python/python-func-open.html)。
encoding="utf-8" 文件编码。
